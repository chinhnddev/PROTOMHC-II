# Experiment: ESM2 Frozen Transformer (tuned)

defaults:
  - _self_

# Hydra output location (per-experiment run folder)
hydra:
  run:
    dir: outputs/${name}
  output_subdir: null

# Experiment info
name: ESM2_Frozen_Transformer
batch_size: 128         # effective 256 with grad accumulation
epochs: 30
min_epochs: 10
seed: 42
log_every_n_steps: 50

# Model parameters
model:
  _target_: src.models.esm2_frozen_transformer.ESM2FrozenTransformer
  num_layers: 4         # smaller head to reduce overfit
  lr: 0.00005           # lower LR for stability
  pos_weight: 6.0       # softer class weight to avoid oscillation

# Trainer + Callbacks
trainer:
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  max_epochs: 30
  min_epochs: 10
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2   # effective batch ~256
  enable_checkpointing: true
  default_root_dir: checkpoints/${name}

  callbacks:
    early_stopping:
      _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: val_auroc
      mode: max
      patience: 7
      min_delta: 0.001
      verbose: true

    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_auroc
      mode: max
      save_top_k: 1
      filename: "esm2_trans-{epoch:02d}-{val_auroc:.4f}"
      verbose: true
