# configs/exp/exp04_protbert.yaml
# ProtBERT baseline experiment

defaults:
  - /model: protbert_transformer
  - _self_

name: ProtBERT_Transformer
batch_size: 128
epochs: 30
min_epochs: 10
seed: 42
log_every_n_steps: 50

trainer:
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  max_epochs: 30
  min_epochs: 10
  gradient_clip_val: 1.0
  enable_checkpointing: true
  default_root_dir: checkpoints/${exp.name}

  callbacks:
    early_stopping:
      _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: val_auroc
      mode: max
      patience: 7
      min_delta: 0.001
      verbose: true

    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_auroc
      mode: max
      save_top_k: 1
      filename: "protbert-{epoch:02d}-{val_auroc:.4f}"
      verbose: true
